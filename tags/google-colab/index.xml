<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Google Colab on 識之箱庭</title><link>https://HoshikawaRyuukou.github.io/tags/google-colab/</link><description>Recent content in Google Colab on 識之箱庭</description><generator>Hugo</generator><language>zh-tw</language><copyright>HoshikawaRyuukou</copyright><lastBuildDate>Thu, 16 Jan 2025 21:11:00 +0000</lastBuildDate><atom:link href="https://HoshikawaRyuukou.github.io/tags/google-colab/index.xml" rel="self" type="application/rss+xml"/><item><title>AI - Ollama - Google Colab + ngrok</title><link>https://HoshikawaRyuukou.github.io/posts/ai---ollama---google-colab-+-ngrok/</link><pubDate>Thu, 16 Jan 2025 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---ollama---google-colab-+-ngrok/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;p>參考以下教學&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=ZOCY61424JI">十分钟部署本地离线免费大模型！&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=JfI3K3HwQuI">Ngrok + Ollama | 在世界任何地方与localhost开源大模型聊天&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.gopenai.com/free-inference-is-all-i-need-how-to-run-large-language-models-for-free-using-google-colab-fe961e86503b">Free Inference Is All I Need: How to Run Large Language Models for Free Using Google Colab&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;ul>
&lt;li>註冊 &lt;a href="https://ngrok.com/">ngrok&lt;/a> 帳號，取得 token ( ngrok &amp;gt; Your Authtoken )&lt;/li>
&lt;li>將 token 填至 colab &amp;gt; Secret
&lt;ul>
&lt;li>name : NGROK_AUTH&lt;/li>
&lt;li>value : token&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>本機端使用 &lt;a href="https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo">Page Assist - A Web UI for Local AI Models&lt;/a> 與 Ollama 互動&lt;/li>
&lt;/ul>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;h3 id="安裝必要工具">安裝必要工具&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!sudo apt-get install -y pciutils
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>!curl https://ollama.ai/install.sh &lt;span style="color:#111">|&lt;/span> sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>!pip install pyngrok
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>安裝 pciutils&lt;/strong>: 提供硬件檢測和配置工具，用於檢查和診斷 GPU 設置。&lt;/li>
&lt;li>&lt;strong>安裝 Ollama&lt;/strong>: 下載並執行 Ollama 的安裝腳本。&lt;/li>
&lt;li>&lt;strong>安裝 pyngrok&lt;/strong>: 用於創建到本地服務的反向代理，從而將本地服務器公開到互聯網。&lt;/li>
&lt;/ul>
&lt;h3 id="啟動-ollama-服務器">啟動 Ollama 服務器&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">os&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">threading&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">subprocess&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">requests&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">pyngrok&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">ngrok&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">google.colab&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">userdata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">ollama&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_HOST&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;0.0.0.0:11434&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_ORIGINS&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;*&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_KEEP_ALIVE&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;-1&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">subprocess&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Popen&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#d88200">&amp;#34;ollama&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;serve&amp;#34;&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_thread&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">threading&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Thread&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">target&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">ollama&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_thread&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">start&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>配置環境變量&lt;/strong>：
&lt;ul>
&lt;li>OLLAMA_HOST: 指定服務器的主機和端口，這裡為 0.0.0.0:11434，表示本地所有網絡接口。&lt;/li>
&lt;li>OLLAMA_ORIGINS: 設置跨域資源共享 (CORS) 的允許範圍。&lt;/li>
&lt;li>OLLAMA_KEEP_ALIVE: 保持服務器活躍的時長（-1 表示無限）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>啟動 Ollama 服務器&lt;/strong>：使用 subprocess 啟動 Ollama 的服務模式。&lt;/li>
&lt;li>&lt;strong>使用執行緒運行服務器&lt;/strong>：確保主程序不被阻塞，允許服務器在後台運行。&lt;/li>
&lt;/ul>
&lt;h3 id="下載模型">下載模型&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://ollama.com/search">Ollama search&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama pull &lt;span style="color:#f92672">{&lt;/span>model_name&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="公開服務">公開服務&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">conf&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">get_default&lt;/span>&lt;span style="color:#111">()&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">auth_token&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">userdata&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">get&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#39;NGROK_AUTH&amp;#39;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_tunnel&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ngrok&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">connect&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#34;11434&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;http&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">public_url&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ollama_tunnel&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">public_url&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;Public URL: &lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">public_url&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>配置 ngrok 驗證令牌&lt;/strong>：使用用戶提供的 NGROK_AUTH 確保 Tunnel 服務的授權。&lt;/li>
&lt;li>&lt;strong>創建 ngrok Tunnel&lt;/strong>： 將本地服務器（11434 端口）通過 HTTP 隧道公開到互聯網。&lt;/li>
&lt;li>&lt;strong>獲取公開 URL&lt;/strong>： 輸出 Tunnel 的公開 URL，便於遠程訪問 Ollama 服務。&lt;/li>
&lt;/ul>
&lt;h3 id="列出可用模型">列出可用模型&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama list
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="執行模型">執行模型&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama run &lt;span style="color:#f92672">{&lt;/span>model_name&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="透過-page-assist-訪問">透過 Page Assist 訪問&lt;/h3>
&lt;ul>
&lt;li>於 Page Assist 設置 public_url&lt;/li>
&lt;li>訪問 public_url 並點擊 visit site，否則 Page Assist 偵測不到遠端 ollama&lt;/li>
&lt;/ul></description></item></channel></rss>