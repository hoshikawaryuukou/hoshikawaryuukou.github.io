<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama on 識之箱庭</title><link>https://HoshikawaRyuukou.github.io/tags/ollama/</link><description>Recent content in Ollama on 識之箱庭</description><generator>Hugo -- gohugo.io</generator><language>zh-tw</language><copyright>HoshikawaRyuukou</copyright><lastBuildDate>Thu, 16 Jan 2025 21:11:00 +0000</lastBuildDate><atom:link href="https://HoshikawaRyuukou.github.io/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>AI - Ollama - Google Colab + ngrok</title><link>https://HoshikawaRyuukou.github.io/posts/ai-ollama-google-colab-+-ngrok/</link><pubDate>Thu, 16 Jan 2025 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai-ollama-google-colab-+-ngrok/</guid><description>Quick Chat 參考以下教學
十分钟部署本地离线免费大模型！ Ngrok + Ollama | 在世界任何地方与localhost开源大模型聊天 Free Inference Is All I Need: How to Run Large Language Models for Free Using Google Colab Requirements 註冊 ngrok 帳號，取得 token ( ngrok &amp;gt; Your Authtoken ) 將 token 填至 colab &amp;gt; Secret name : NGROK_AUTH value : token 本機端使用 Page Assist - A Web UI for Local AI Models 與 Ollama 互動 Steps 安裝必要工具 !sudo apt-get install -y pciutils !</description></item><item><title>AI - Ollama - Note</title><link>https://HoshikawaRyuukou.github.io/posts/ai-ollama-note/</link><pubDate>Mon, 19 Aug 2024 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai-ollama-note/</guid><description>Guide Ollama UI Page Assist - A Web UI for Local AI Models Commands ollama list : 查看以配置本地模型 ollama run {model} : 下載/執行模型 ollama ps : 展示目前載入的模型、它們所佔的記憶體大小以及所使用的處理器類型（GPU 或 CPU） Use model from Ollama Ollama search Use GGUF model from Hugging Face Hub # Run Ollama with specified model # ollama run hf.co/{username}/{repository} ollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF # Run Ollama with specified model and desired quantization # ollama run hf.co/{username}/{repository}:{quantization} ollama run hf.</description></item></channel></rss>