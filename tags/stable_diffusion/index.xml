<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Stable_Diffusion on 識之箱庭</title><link>https://HoshikawaRyuukou.github.io/tags/stable_diffusion/</link><description>Recent content in Stable_Diffusion on 識之箱庭</description><generator>Hugo</generator><language>zh-tw</language><copyright>HoshikawaRyuukou</copyright><lastBuildDate>Thu, 12 Jun 2025 10:00:00 +0800</lastBuildDate><atom:link href="https://HoshikawaRyuukou.github.io/tags/stable_diffusion/index.xml" rel="self" type="application/rss+xml"/><item><title>Stable Diffusion - Inpainting</title><link>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---inpainting/</link><pubDate>Thu, 12 Jun 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---inpainting/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;目前並未特別使用進階修圖技巧。若圖片瑕疵可透過簡單塗色與描邊處理，即會嘗試修復。&lt;/p&gt;
&lt;p&gt;若瑕疵較嚴重，則多半直接放棄並重新生成 —— 通常下一張會更好。&lt;/p&gt;
&lt;h2 id="modification"&gt;Modification&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;圖片先送入 &lt;code&gt;img2img&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;簡單處理：使用 &lt;code&gt;Inpaint sketch&lt;/code&gt;（注意不要在 sketch 模式下直接生成）。&lt;/li&gt;
&lt;li&gt;複雜處理：使用 &lt;code&gt;photopea-embed&lt;/code&gt; 進行手動遮罩或編輯。&lt;/li&gt;
&lt;li&gt;完成後再送回 &lt;code&gt;Inpaint&lt;/code&gt; 重新生成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="generation"&gt;Generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;採樣方法（Sampling method）：&lt;code&gt;DPM++ 2M&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;時間表類型（Schedule type）：&lt;code&gt;Karras&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;去雜強度（Denoising strength）建議範圍：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;0.2 ~ 0.3&lt;/code&gt;：保留原圖整體色彩結構，僅微調瑕疵與過渡區域。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;0.3 ~ 0.5&lt;/code&gt;：適度改變結構與細節，適合嘗試新的構圖或調整 seed 取得更好結果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Stable Diffusion - Quick Start</title><link>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---quick-start/</link><pubDate>Mon, 17 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---quick-start/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;⚠️ 這是一篇新手導引，目的不在於精準解釋。&lt;/li&gt;
&lt;li&gt;⚠️ &lt;strong&gt;Checkpoint&lt;/strong&gt; 一般會提供推薦的參數設置，建議依據模型的特性調整，以獲得最佳效果。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="resources"&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://civitai.com/"&gt;Civitai: The Home of Open-Source Generative AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="checkpoint"&gt;Checkpoint&lt;/h2&gt;
&lt;p&gt;決定生成圖片的基礎風格。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;寫實風格 (Photorealistic)&lt;/li&gt;
&lt;li&gt;動漫風 (Anime)&lt;/li&gt;
&lt;li&gt;油畫風格 (Painting)&lt;/li&gt;
&lt;li&gt;科幻賽博龐克 (Cyberpunk)&lt;/li&gt;
&lt;li&gt;像素風格 (Pixel Art)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lora"&gt;LoRA&lt;/h2&gt;
&lt;p&gt;輕量化微調模型可額外載入來增強特定風格或角色。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;簡單的比喻來形容 LoRA 模型，那就是「濾鏡」&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="embedding"&gt;Embedding&lt;/h2&gt;
&lt;p&gt;增強對某些 Prompt 的理解。&lt;/p&gt;
&lt;h2 id="vae"&gt;VAE&lt;/h2&gt;
&lt;p&gt;提高圖片細節與顏色準確度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;📝 部分 Checkpoints 會內建（Baked）VAE，如使用外部 VAE，請確認是否需要覆蓋內建版本。&lt;/li&gt;
&lt;li&gt;⚠️ 如果發現圖片的型都對，但只有顏色壞掉，通常都是 VAE 的問題。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="resolutions"&gt;Resolutions&lt;/h2&gt;
&lt;p&gt;不同種類的 Checkpoints 建議的解析度會有所不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SD 1.5
&lt;ul&gt;
&lt;li&gt;512 x 512 : 1:1&lt;/li&gt;
&lt;li&gt;512 X 768 : 2:3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SDXL
&lt;ul&gt;
&lt;li&gt;640 x 1536 = 5:12&lt;/li&gt;
&lt;li&gt;768 x 1344 = 4:7&lt;/li&gt;
&lt;li&gt;832 x 1216 = 13:19&lt;/li&gt;
&lt;li&gt;896 x 1152 = 7:9&lt;/li&gt;
&lt;li&gt;1024 x 1024 = 1:1&lt;/li&gt;
&lt;li&gt;1024 x 1536 = 2:3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sampler--schedule"&gt;Sampler + Schedule&lt;/h2&gt;
&lt;p&gt;Sampler 是從雜訊圖到成品的&lt;strong&gt;去噪算法&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>Stable Diffusion - Env</title><link>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---env/</link><pubDate>Mon, 10 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---env/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;⚠️ 以下皆須安裝指定版本不可貿然升級。&lt;/p&gt;
&lt;p&gt;目前產圖只使用到 Forge + SDXL 。&lt;/p&gt;
&lt;h2 id="local-deployment"&gt;Local deployment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nvidia 驅動更新至最新&lt;/li&gt;
&lt;li&gt;檢查顯卡支援的最高 cuda 支援: &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安裝 &lt;a href="https://developer.nvidia.com/cuda-12-1-0-download-archive"&gt;CUDA 12.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;顯示 cuda 編譯工具的版本信息: &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安裝 [&lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;stable-diffusion-webui-forge&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="google-colab"&gt;Google Colab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gutris1/segsmaker"&gt;gutris1/segsmaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;配置 Civitai API Key
&lt;ul&gt;
&lt;li&gt;Civitai 網站 Menu &amp;gt; Account Settings(齒輪 icon) &amp;gt; API Keys&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="extensions"&gt;Extensions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DominikDoom/a1111-sd-webui-tagcomplete"&gt;DominikDoom/a1111-sd-webui-tagcomplete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Physton/sd-webui-prompt-all-in-one"&gt;Physton/sd-webui-prompt-all-in-one&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Bing-su/adetailer"&gt;Bing-su/adetailer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yankooliveira/sd-webui-photopea-embed"&gt;yankooliveira/sd-webui-photopea-embed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Stable Diffusion</title><link>https://HoshikawaRyuukou.github.io/posts/illustrious-xl/</link><pubDate>Thu, 06 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/illustrious-xl/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;目前 Stable Diffusion 只拿來自娛自樂 😃。&lt;/p&gt;
&lt;h2 id="core-working-principles"&gt;Core Working Principles&lt;/h2&gt;
&lt;p&gt;Stable Diffusion 主要包含三個核心技術：&lt;/p&gt;
&lt;h3 id="前向擴散forward-diffusion"&gt;前向擴散（Forward Diffusion）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;先從大量圖片資料集中學習圖片特徵。&lt;/li&gt;
&lt;li&gt;然後，系統會逐步加入高斯雜訊（Gaussian Noise），使圖片變得模糊、無法辨識。&lt;/li&gt;
&lt;li&gt;最後，這個過程會讓圖片變成完全的純雜訊（random noise）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="反向去噪reverse-denoising--u-net"&gt;反向去噪（Reverse Denoising / U-Net）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion 學習如何逆向去噪，一步步從雜訊還原出清晰的圖片。&lt;/li&gt;
&lt;li&gt;這部分的關鍵是 U-Net 神經網路架構，它可以在多層次的細節中，捕捉圖片的各種特徵。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="文本引導text-conditioning--clip"&gt;文本引導（Text Conditioning / CLIP）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion 之所以能生成符合指令的圖片，是因為它使用了CLIP（Contrastive Language-Image Pretraining）。&lt;/li&gt;
&lt;li&gt;CLIP 會將文字轉換成向量表示（latent embeddings），這些向量再指導模型生成符合描述的圖像。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="diagram"&gt;Diagram&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/"&gt;Improving Diffusion Models as an Alternative To GANs, Part 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
 &lt;img src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/Generation-with-Diffusion-Models.png" alt="123"&gt;

&lt;/p&gt;
&lt;h2 id="extras"&gt;Extras&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pttweb.cc/bbs/C_Chat/M.1730732828.A.70C"&gt;Re: [問題] AI 風格怎麼了嗎？為什麼容易膩？ - 看板C_Chat - PTT網頁版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>