<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 識之箱庭</title><link>https://HoshikawaRyuukou.github.io/tags/ai/</link><description>Recent content in AI on 識之箱庭</description><generator>Hugo</generator><language>zh-tw</language><copyright>HoshikawaRyuukou</copyright><lastBuildDate>Fri, 24 Oct 2025 11:41:40 +0800</lastBuildDate><atom:link href="https://HoshikawaRyuukou.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>AI Coding - Spec-Driven Development (SDD)</title><link>https://HoshikawaRyuukou.github.io/posts/ai-coding---spec-driven-development-sdd/</link><pubDate>Fri, 24 Oct 2025 11:41:40 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai-coding---spec-driven-development-sdd/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;首先，&lt;strong&gt;規格驅動開發（SDD）並非在 AI 程式碼生成技術出現後才被提出&lt;/strong&gt;，它是一個早已存在的概念。&lt;/p&gt;
&lt;p&gt;然而，在當前環境下，SDD 重新受到關注：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;成本驟降&lt;/strong&gt;：AI 的輔助極大地降低了編寫高品質規格文件的成本和時間。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解決困境&lt;/strong&gt;：業界普遍遭遇「Vibe Coding」（憑感覺、缺乏規範的開發）所帶來的維護性和協作性困境。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;回歸檯面&lt;/strong&gt;：市場開始追求更嚴謹、更有規範的寫作方式，使規格驅動開發重新成為主流討論。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我首次被這個概念震撼，是在 &lt;strong&gt;Kiro IDE&lt;/strong&gt; 的演示中。儘管 Kiro 以搶先體驗（Early Access）方式開放，許多開發者無法接觸（需要邀請碼），但這反而間接促成了開源社群的迅速響應。如今，相關工具如雨後春筍般出現，&lt;strong&gt;Kiro 似乎已丟失了先發優勢&lt;/strong&gt;。😂&lt;/p&gt;
&lt;h2 id="guide"&gt;Guide&lt;/h2&gt;
&lt;p&gt;⚠️ 建議讀者著重理解這些資源所闡述的&lt;strong&gt;核心設計思路&lt;/strong&gt;，而非具體的技術實現，因為相關的工具與生態系統正在快速變化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/github/spec-kit/blob/main/spec-driven.md"&gt;spec-kit/spec-driven.md at main · github/spec-kit · GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.blog/ai-and-ml/generative-ai/spec-driven-development-using-markdown-as-a-programming-language-when-building-with-ai/"&gt;Spec-driven development: Using Markdown as a programming language when building with AI - The GitHub Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html"&gt;Understanding Spec-Driven-Development: Kiro, spec-kit, and Tessl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="concept"&gt;Concept&lt;/h2&gt;
&lt;p&gt;SDD 的核心在於&lt;strong&gt;顛覆軟體開發的傳統權力結構&lt;/strong&gt;，並將 &lt;strong&gt;規格說明書（Specification）&lt;/strong&gt; 提升為開發過程的 &lt;strong&gt;核心真理來源 (Source of Truth)&lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;權力翻轉&lt;/strong&gt;：規格說明書不再是服務程式碼的附屬文件，而是&lt;strong&gt;程式碼必須服務規格說明書&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;真理來源&lt;/strong&gt;：產品需求文件（PRD）不再僅是實作的&lt;strong&gt;指南&lt;/strong&gt;，而是&lt;strong&gt;生成實作的源頭&lt;/strong&gt;。技術規格成為精確的定義，可以直接&lt;strong&gt;驅動程式碼的生成&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;消除鴻溝&lt;/strong&gt;：SDD 透過使規格說明及其具體的實作計畫&lt;strong&gt;可執行化 (Executable)&lt;/strong&gt;，從而&lt;strong&gt;消除了規格說明與實際程式碼之間的鴻溝&lt;/strong&gt;。當規格說明直接生成程式碼時，這不再是手動實作，而是一個&lt;strong&gt;純粹的轉化 (Transformation) 過程&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="thinking"&gt;Thinking&lt;/h2&gt;
&lt;p&gt;在採用 SDD 之前，我們需要先自問：&lt;strong&gt;你是否希望在一個完全由 AI 主導的結構中工作，每次迭代都只透過修改文件來完成？&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>AI Coding - SDD - OpenSpec</title><link>https://HoshikawaRyuukou.github.io/posts/ai-coding---sdd---openspec/</link><pubDate>Fri, 17 Oct 2025 14:12:33 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai-coding---sdd---openspec/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;在使用 OpenSpec 進行開發的體驗相當不錯，我認為原作者的設計理念解決了許多團隊在導入 AI 協作時的痛點。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenSpec 的核心優勢：&lt;/strong&gt; 它極其擅長管理&lt;strong&gt;既有功能的變更 (1→n)&lt;/strong&gt;。 OpenSpec 是一個&lt;strong&gt;輕量級&lt;/strong&gt;的框架，無需 API 金鑰即可輕鬆整合到現有專案中，不需從頭來過。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;現有工具的痛點：&lt;/strong&gt; 許多工具如 &lt;code&gt;spec-kit&lt;/code&gt; 或 &lt;code&gt;Kiro&lt;/code&gt; 在處理&lt;strong&gt;從零到一 (0→1) 的全新專案&lt;/strong&gt;時表現非常出色。 然而，當專案進入維護與迭代階段，需要持續更新和管理不斷演進的「規格文件」時，這些工具就顯得力不從心。它們的結構在追蹤「系統當前狀態」以及「即將發生的變更」時，往往不夠直觀。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;清晰的檔案結構：&lt;/strong&gt; OpenSpec 透過簡單的雙資料夾結構，讓規格與變更的管理一目了然，有效地將「事實的根源」與「進行中的提案」分開：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;specs/&lt;/code&gt; → &lt;strong&gt;系統的當前狀態&lt;/strong&gt;：存放目前系統的規格，是團隊與 AI 的共同認知基礎。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;changes/&lt;/code&gt; → &lt;strong&gt;進行中的變更&lt;/strong&gt;：所有新的功能需求或修改都會在這裡以獨立的資料夾進行，包含提案、任務列表與規格草案。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;archive/&lt;/code&gt; → &lt;strong&gt;已完成的變更歷史&lt;/strong&gt;：所有已合併的變更都會被移至此處，成為一份「活文件」，記錄系統的演進歷程。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="guide"&gt;Guide&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Fission-AI/OpenSpec"&gt;GitHub - Fission-AI/OpenSpec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=ANjiJQQIBo0"&gt;開發者福音！用 AI 迭代現有專案？OpenSpec 讓 AI 按規範寫程式碼，零失誤！&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一位開發者的使用心得分享，原作者也在評論區參與了討論，提供了更多有趣的見解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=cQv3ocbsKHY"&gt;I Found the Simplest AI Dev Tool Ever&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="workflow"&gt;Workflow&lt;/h2&gt;
&lt;p&gt;這個流程確保了人類開發者與 AI 之間能夠進行順暢且可控的協作，在 AI 動手寫程式碼&lt;strong&gt;之前&lt;/strong&gt;，就先對齊目標。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;起草變更提案 (Draft Change Proposal)：&lt;/strong&gt; 開發者向 AI 提出一個新的功能需求或變更請求。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;審查與對齊 (Review &amp;amp; Align)：&lt;/strong&gt; AI 會根據你的請求生成詳細的規格與任務列表。開發者此時可以審查、提出修改意見，AI 會再根據回饋更新，形成一個&lt;strong&gt;反覆回饋的循環&lt;/strong&gt;，直到雙方對「要做什麼」與「如何做」達成共識。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;執行任務 (Implement Tasks)：&lt;/strong&gt; AI 依據最終拍板定案的規格與任務列表，開始進行程式碼的編寫。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;套用變更 (Apply the Change)：&lt;/strong&gt; 開發者審查 AI 生成的程式碼，確認無誤後，執行指令將程式碼合併到專案中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;封存與更新 (Archive &amp;amp; Update Specs)：&lt;/strong&gt; 變更完成後，該次的變更紀錄會被完整地「封存」到 &lt;code&gt;archive/&lt;/code&gt; 資料夾，同時更新 &lt;code&gt;specs/&lt;/code&gt; 中的主要規格文件，確保規格文件永遠處於最新狀態，為下一次的變更做好準備。&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>Gemini - Google AI Studio</title><link>https://HoshikawaRyuukou.github.io/posts/gemini---google-ai-studio/</link><pubDate>Wed, 15 Oct 2025 15:17:27 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/gemini---google-ai-studio/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;Google AI Studio 是 Google 推出的線上平台，主要定位在協助開發者、學生與研究人員快速試用 &lt;strong&gt;Gemini 模型&lt;/strong&gt;，並透過 &lt;strong&gt;Gemini Developer API&lt;/strong&gt; 來建立原型或應用程式。&lt;/p&gt;
&lt;p&gt;與一般的 Gemini 應用（像 ChatGPT 類型的介面）相比，AI Studio 提供了更多的 &lt;strong&gt;免費額度&lt;/strong&gt;，對想要動手實驗 Gemini 功能的人來說相當友善。&lt;/p&gt;
&lt;h2 id="guide"&gt;Guide&lt;/h2&gt;
&lt;h3 id="chat"&gt;Chat&lt;/h3&gt;
&lt;p&gt;常用的控制項包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Grounding with Google Search&lt;/strong&gt;：可在回答前透過 Google 搜尋補強資訊，使回覆更準確。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;URL context&lt;/strong&gt;：可將指定連結內容作為上下文，讓回答更貼近指定網頁的資訊。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="build"&gt;Build&lt;/h3&gt;
&lt;p&gt;這個區域能將「文字提示」轉化為可執行的 Web 應用程式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Vibe Coding&lt;/strong&gt; 功能在開發簡單原型或概念驗證（PoC）時相當好用。&lt;/li&gt;
&lt;li&gt;不過在進行功能迭代時，有時會導致整個結構被重新生成 😂。&lt;/li&gt;
&lt;li&gt;若只是開發「自用型」應用程式（不綁 API Key），頂多遇到免費流量上限就無法繼續使用；&lt;/li&gt;
&lt;li&gt;若要分享或部署給他人，則需要考慮 &lt;strong&gt;API 使用費用&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前開發了幾個小應用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gemini 圖片翻譯器：使用 Gemini 翻譯圖片而不是 Google Translate。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="notice"&gt;Notice&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;若開啟歷史紀錄功能，AI Studio 的紀錄會自動儲存在 &lt;strong&gt;Google Drive&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;由於目前平台 &lt;strong&gt;未提供批次刪除功能&lt;/strong&gt;，若想清空紀錄，必須手動至 Google Drive 操作。&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>AI - Prompt - Headshot</title><link>https://HoshikawaRyuukou.github.io/posts/ai---prompt---headshot/</link><pubDate>Sat, 27 Sep 2025 13:55:55 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---prompt---headshot/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;最近為了更新履歷需要一張專業大頭照，但我手邊只有生活照。於是我使用 Google AI Studio 上的 &lt;strong&gt;Gemini 2.5 Flash Image (Nano Banana)&lt;/strong&gt;，效果相當不錯。&lt;/p&gt;
&lt;h2 id="practice"&gt;Practice&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;服裝&lt;/strong&gt;：深色西裝 &lt;br&gt;
&lt;strong&gt;光線&lt;/strong&gt;：側光，額頭去除油光 &lt;br&gt;
&lt;strong&gt;構圖&lt;/strong&gt;：半身照，臉部佔 70%~80% &lt;br&gt;
&lt;strong&gt;尺寸&lt;/strong&gt;：4.5 公分 × 3.5 公分 &lt;br&gt;
&lt;strong&gt;表情&lt;/strong&gt;：微笑但自然，嘴巴合閉不露齒 &lt;br&gt;
&lt;strong&gt;背景&lt;/strong&gt;：白色，光源均勻，無陰影或反光 &lt;br&gt;
&lt;strong&gt;品質&lt;/strong&gt;：清晰對焦，高品質無摺痕 &lt;br&gt;
&lt;strong&gt;角度&lt;/strong&gt;：眼睛正視鏡頭&lt;/p&gt;
&lt;p&gt;⚠️ 關於&lt;strong&gt;尺寸&lt;/strong&gt;的指令（如 4.5cm × 3.5cm），目前的 AI 模型通常無法直接生成精確的實體尺寸。 加入這項指令主要是為了讓 AI 更理解「這是一張證件照」的脈絡，從而優化構圖與臉部比例。最終，你仍需要將生成圖片匯入影像編輯軟體進行手動裁切，以符合最終需求。&lt;/p&gt;</description></item><item><title>AI - Favorites</title><link>https://HoshikawaRyuukou.github.io/posts/ai---favorites/</link><pubDate>Fri, 05 Sep 2025 10:25:42 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---favorites/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;記錄些目前使用率很高的 AI 工具 🤓。&lt;/p&gt;
&lt;h2 id="collections"&gt;Collections&lt;/h2&gt;
&lt;h3 id="chat"&gt;Chat&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://aistudio.google.com/prompts/new_chat"&gt;Google AI Studio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://chatgpt.com/"&gt;ChatGPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gemini.google.com/app?hl=zh-TW"&gt;Google Gemini&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://grok.com/"&gt;Grok&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="search"&gt;Search&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://felo.ai/zh-Hant/search"&gt;Felo - 您的免費 AI 搜尋引擎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="all-in-one"&gt;All-in-One&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.genspark.ai/"&gt;Genspark - 一站式 AI 工作空間&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="design"&gt;Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.lovart.ai/zh-TW"&gt;Lovart | 全球首個設計智能體&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=XtWHGmEz5CA"&gt;🔥Google Stitch颠覆传统UI设计！10秒生成专业级UI！快速生成产品原型！小白也能开发精美UI。 支持无缝导入Figma！ Stitch保姆级教程：从想法到APP大师级界面效果堪比专业团队 - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>AI - Tools</title><link>https://HoshikawaRyuukou.github.io/posts/ai---tools/</link><pubDate>Fri, 29 Aug 2025 10:08:39 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---tools/</guid><description>&lt;h2 id="news"&gt;News&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.aibase.com/zh/"&gt;AIbase基地 - 让更多人看到未来 通往AGI之路&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="audio"&gt;Audio&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV17SVUznEGw"&gt;AI声音建模：MiniMax Audio 一键声音克隆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="coding"&gt;Coding&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=IqVo8V4QNm0"&gt;GitHub项目理解神器：DeepWiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/AsyncFuncAI/deepwiki-open"&gt;AsyncFuncAI/deepwiki-open&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="design"&gt;Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=c0K308mLz8U"&gt;【Lovart】設計師偷偷在用的AI 工具🫢 3分鐘搞定 Logo＋海報＋網站🔥 Laichu - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="translation"&gt;Translation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/SakuraLLM/SakuraLLM"&gt;SakuraLLM/SakuraLLM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zyddnys/manga-image-translator"&gt;zyddnys/manga-image-translator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmMaze/BallonsTranslator"&gt;dmMaze/BallonsTranslator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Stable Diffusion - Inpainting</title><link>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---inpainting/</link><pubDate>Thu, 12 Jun 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---inpainting/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;目前並未特別使用進階修圖技巧。若圖片瑕疵可透過簡單塗色與描邊處理，即會嘗試修復。&lt;/p&gt;
&lt;p&gt;若瑕疵較嚴重，則多半直接放棄並重新生成 —— 通常下一張會更好。&lt;/p&gt;
&lt;h2 id="modification"&gt;Modification&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;圖片先送入 &lt;code&gt;img2img&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;簡單處理：使用 &lt;code&gt;Inpaint sketch&lt;/code&gt;（注意不要在 sketch 模式下直接生成）。&lt;/li&gt;
&lt;li&gt;複雜處理：使用 &lt;code&gt;photopea-embed&lt;/code&gt; 進行手動遮罩或編輯。&lt;/li&gt;
&lt;li&gt;完成後再送回 &lt;code&gt;Inpaint&lt;/code&gt; 重新生成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="generation"&gt;Generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;採樣方法（Sampling method）：&lt;code&gt;DPM++ 2M&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;時間表類型（Schedule type）：&lt;code&gt;Karras&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;去雜強度（Denoising strength）建議範圍：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;0.2 ~ 0.3&lt;/code&gt;：保留原圖整體色彩結構，僅微調瑕疵與過渡區域。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;0.3 ~ 0.5&lt;/code&gt;：適度改變結構與細節，適合嘗試新的構圖或調整 seed 取得更好結果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Stable Diffusion - Quick Start</title><link>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---quick-start/</link><pubDate>Mon, 17 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---quick-start/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;⚠️ 這是一篇新手導引，目的不在於精準解釋。&lt;/li&gt;
&lt;li&gt;⚠️ &lt;strong&gt;Checkpoint&lt;/strong&gt; 一般會提供推薦的參數設置，建議依據模型的特性調整，以獲得最佳效果。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="resources"&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://civitai.com/"&gt;Civitai: The Home of Open-Source Generative AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="checkpoint"&gt;Checkpoint&lt;/h2&gt;
&lt;p&gt;決定生成圖片的基礎風格。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;寫實風格 (Photorealistic)&lt;/li&gt;
&lt;li&gt;動漫風 (Anime)&lt;/li&gt;
&lt;li&gt;油畫風格 (Painting)&lt;/li&gt;
&lt;li&gt;科幻賽博龐克 (Cyberpunk)&lt;/li&gt;
&lt;li&gt;像素風格 (Pixel Art)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lora"&gt;LoRA&lt;/h2&gt;
&lt;p&gt;輕量化微調模型可額外載入來增強特定風格或角色。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;簡單的比喻來形容 LoRA 模型，那就是「濾鏡」&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="embedding"&gt;Embedding&lt;/h2&gt;
&lt;p&gt;增強對某些 Prompt 的理解。&lt;/p&gt;
&lt;h2 id="vae"&gt;VAE&lt;/h2&gt;
&lt;p&gt;提高圖片細節與顏色準確度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;📝 部分 Checkpoints 會內建（Baked）VAE，如使用外部 VAE，請確認是否需要覆蓋內建版本。&lt;/li&gt;
&lt;li&gt;⚠️ 如果發現圖片的型都對，但只有顏色壞掉，通常都是 VAE 的問題。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="resolutions"&gt;Resolutions&lt;/h2&gt;
&lt;p&gt;不同種類的 Checkpoints 建議的解析度會有所不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SD 1.5
&lt;ul&gt;
&lt;li&gt;512 x 512 : 1:1&lt;/li&gt;
&lt;li&gt;512 X 768 : 2:3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SDXL
&lt;ul&gt;
&lt;li&gt;640 x 1536 = 5:12&lt;/li&gt;
&lt;li&gt;768 x 1344 = 4:7&lt;/li&gt;
&lt;li&gt;832 x 1216 = 13:19&lt;/li&gt;
&lt;li&gt;896 x 1152 = 7:9&lt;/li&gt;
&lt;li&gt;1024 x 1024 = 1:1&lt;/li&gt;
&lt;li&gt;1024 x 1536 = 2:3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sampler--schedule"&gt;Sampler + Schedule&lt;/h2&gt;
&lt;p&gt;Sampler 是從雜訊圖到成品的&lt;strong&gt;去噪算法&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>Stable Diffusion - Env</title><link>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---env/</link><pubDate>Mon, 10 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/stable-diffusion---env/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;⚠️ 以下皆須安裝指定版本不可貿然升級。&lt;/p&gt;
&lt;p&gt;目前產圖只使用到 Forge + SDXL 。&lt;/p&gt;
&lt;h2 id="local-deployment"&gt;Local deployment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nvidia 驅動更新至最新&lt;/li&gt;
&lt;li&gt;檢查顯卡支援的最高 cuda 支援: &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安裝 &lt;a href="https://developer.nvidia.com/cuda-12-1-0-download-archive"&gt;CUDA 12.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;顯示 cuda 編譯工具的版本信息: &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安裝 [&lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;stable-diffusion-webui-forge&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="google-colab"&gt;Google Colab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gutris1/segsmaker"&gt;gutris1/segsmaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;配置 Civitai API Key
&lt;ul&gt;
&lt;li&gt;Civitai 網站 Menu &amp;gt; Account Settings(齒輪 icon) &amp;gt; API Keys&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="extensions"&gt;Extensions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DominikDoom/a1111-sd-webui-tagcomplete"&gt;DominikDoom/a1111-sd-webui-tagcomplete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Physton/sd-webui-prompt-all-in-one"&gt;Physton/sd-webui-prompt-all-in-one&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Bing-su/adetailer"&gt;Bing-su/adetailer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yankooliveira/sd-webui-photopea-embed"&gt;yankooliveira/sd-webui-photopea-embed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Stable Diffusion</title><link>https://HoshikawaRyuukou.github.io/posts/illustrious-xl/</link><pubDate>Thu, 06 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/illustrious-xl/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;目前 Stable Diffusion 只拿來自娛自樂 😃。&lt;/p&gt;
&lt;h2 id="core-working-principles"&gt;Core Working Principles&lt;/h2&gt;
&lt;p&gt;Stable Diffusion 主要包含三個核心技術：&lt;/p&gt;
&lt;h3 id="前向擴散forward-diffusion"&gt;前向擴散（Forward Diffusion）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;先從大量圖片資料集中學習圖片特徵。&lt;/li&gt;
&lt;li&gt;然後，系統會逐步加入高斯雜訊（Gaussian Noise），使圖片變得模糊、無法辨識。&lt;/li&gt;
&lt;li&gt;最後，這個過程會讓圖片變成完全的純雜訊（random noise）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="反向去噪reverse-denoising--u-net"&gt;反向去噪（Reverse Denoising / U-Net）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion 學習如何逆向去噪，一步步從雜訊還原出清晰的圖片。&lt;/li&gt;
&lt;li&gt;這部分的關鍵是 U-Net 神經網路架構，它可以在多層次的細節中，捕捉圖片的各種特徵。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="文本引導text-conditioning--clip"&gt;文本引導（Text Conditioning / CLIP）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion 之所以能生成符合指令的圖片，是因為它使用了CLIP（Contrastive Language-Image Pretraining）。&lt;/li&gt;
&lt;li&gt;CLIP 會將文字轉換成向量表示（latent embeddings），這些向量再指導模型生成符合描述的圖像。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="diagram"&gt;Diagram&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/"&gt;Improving Diffusion Models as an Alternative To GANs, Part 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
 &lt;img src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/Generation-with-Diffusion-Models.png" alt="123"&gt;

&lt;/p&gt;
&lt;h2 id="extras"&gt;Extras&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pttweb.cc/bbs/C_Chat/M.1730732828.A.70C"&gt;Re: [問題] AI 風格怎麼了嗎？為什麼容易膩？ - 看板C_Chat - PTT網頁版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>