<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 識之箱庭</title><link>https://HoshikawaRyuukou.github.io/tags/ai/</link><description>Recent content in AI on 識之箱庭</description><generator>Hugo</generator><language>zh-tw</language><copyright>HoshikawaRyuukou</copyright><lastBuildDate>Thu, 16 Jan 2025 21:11:00 +0000</lastBuildDate><atom:link href="https://HoshikawaRyuukou.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>AI - Ollama - Google Colab + ngrok</title><link>https://HoshikawaRyuukou.github.io/posts/ai---ollama---google-colab-+-ngrok/</link><pubDate>Thu, 16 Jan 2025 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---ollama---google-colab-+-ngrok/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;p>參考以下教學&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=ZOCY61424JI">十分钟部署本地离线免费大模型！&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=JfI3K3HwQuI">Ngrok + Ollama | 在世界任何地方与localhost开源大模型聊天&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.gopenai.com/free-inference-is-all-i-need-how-to-run-large-language-models-for-free-using-google-colab-fe961e86503b">Free Inference Is All I Need: How to Run Large Language Models for Free Using Google Colab&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;ul>
&lt;li>註冊 &lt;a href="https://ngrok.com/">ngrok&lt;/a> 帳號，取得 token ( ngrok &amp;gt; Your Authtoken )&lt;/li>
&lt;li>將 token 填至 colab &amp;gt; Secret
&lt;ul>
&lt;li>name : NGROK_AUTH&lt;/li>
&lt;li>value : token&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>本機端使用 &lt;a href="https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo">Page Assist - A Web UI for Local AI Models&lt;/a> 與 Ollama 互動&lt;/li>
&lt;/ul>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;h3 id="安裝必要工具">安裝必要工具&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!sudo apt-get install -y pciutils
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>!curl https://ollama.ai/install.sh &lt;span style="color:#111">|&lt;/span> sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>!pip install pyngrok
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>安裝 pciutils&lt;/strong>: 提供硬件檢測和配置工具，用於檢查和診斷 GPU 設置。&lt;/li>
&lt;li>&lt;strong>安裝 Ollama&lt;/strong>: 下載並執行 Ollama 的安裝腳本。&lt;/li>
&lt;li>&lt;strong>安裝 pyngrok&lt;/strong>: 用於創建到本地服務的反向代理，從而將本地服務器公開到互聯網。&lt;/li>
&lt;/ul>
&lt;h3 id="啟動-ollama-服務器">啟動 Ollama 服務器&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">os&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">threading&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">subprocess&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">requests&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">pyngrok&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">ngrok&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">google.colab&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">userdata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">ollama&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_HOST&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;0.0.0.0:11434&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_ORIGINS&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;*&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_KEEP_ALIVE&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;-1&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">subprocess&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Popen&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#d88200">&amp;#34;ollama&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;serve&amp;#34;&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_thread&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">threading&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Thread&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">target&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">ollama&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_thread&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">start&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>配置環境變量&lt;/strong>：
&lt;ul>
&lt;li>OLLAMA_HOST: 指定服務器的主機和端口，這裡為 0.0.0.0:11434，表示本地所有網絡接口。&lt;/li>
&lt;li>OLLAMA_ORIGINS: 設置跨域資源共享 (CORS) 的允許範圍。&lt;/li>
&lt;li>OLLAMA_KEEP_ALIVE: 保持服務器活躍的時長（-1 表示無限）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>啟動 Ollama 服務器&lt;/strong>：使用 subprocess 啟動 Ollama 的服務模式。&lt;/li>
&lt;li>&lt;strong>使用執行緒運行服務器&lt;/strong>：確保主程序不被阻塞，允許服務器在後台運行。&lt;/li>
&lt;/ul>
&lt;h3 id="下載模型">下載模型&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://ollama.com/search">Ollama search&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama pull &lt;span style="color:#f92672">{&lt;/span>model_name&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="公開服務">公開服務&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">conf&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">get_default&lt;/span>&lt;span style="color:#111">()&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">auth_token&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">userdata&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">get&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#39;NGROK_AUTH&amp;#39;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_tunnel&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ngrok&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">connect&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#34;11434&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;http&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">public_url&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ollama_tunnel&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">public_url&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;Public URL: &lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">public_url&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>配置 ngrok 驗證令牌&lt;/strong>：使用用戶提供的 NGROK_AUTH 確保 Tunnel 服務的授權。&lt;/li>
&lt;li>&lt;strong>創建 ngrok Tunnel&lt;/strong>： 將本地服務器（11434 端口）通過 HTTP 隧道公開到互聯網。&lt;/li>
&lt;li>&lt;strong>獲取公開 URL&lt;/strong>： 輸出 Tunnel 的公開 URL，便於遠程訪問 Ollama 服務。&lt;/li>
&lt;/ul>
&lt;h3 id="列出可用模型">列出可用模型&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama list
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="執行模型">執行模型&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama run &lt;span style="color:#f92672">{&lt;/span>model_name&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="透過-page-assist-訪問">透過 Page Assist 訪問&lt;/h3>
&lt;ul>
&lt;li>於 Page Assist 設置 public_url&lt;/li>
&lt;li>訪問 public_url 並點擊 visit site，否則 Page Assist 偵測不到遠端 ollama&lt;/li>
&lt;/ul></description></item><item><title>AI - Prompt Engineering</title><link>https://HoshikawaRyuukou.github.io/posts/ai---prompt-engineering/</link><pubDate>Mon, 18 Nov 2024 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---prompt-engineering/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;p>提示工程是一種專注於設計和優化輸入提示的&lt;strong>操作者技術&lt;/strong>，旨在不改變模型的前提下，通過精心設計提示來提升生成式人工智慧（如大型語言模型，LLMs）的輸出品質。這種技術能幫助模型更準確地理解用戶意圖並生成符合需求的回應。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>提示（Prompt）&lt;/strong>：提示是提供給 AI 模型的輸入內容，如問題、命令或指示。高品質提示是生成高品質輸出的關鍵。&lt;/li>
&lt;/ul>
&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://medium.com/@micky2428/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B-prompt-engineering-%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E8%88%87ai%E5%B0%8D%E8%A9%B1-c4e6501c9bfd">提示工程(Prompt Engineering)：如何有效與AI對話&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="practice---github-copilot">Practice - Github Copilot&lt;/h2>
&lt;h3 id="domain-model">Domain Model&lt;/h3>
&lt;ul>
&lt;li>偏重邏輯，提示詞規格越詳細越好。&lt;/li>
&lt;/ul>
&lt;h3 id="vue-component">Vue Component&lt;/h3>
&lt;ul>
&lt;li>前端組件，初期提示詞規格未必越詳細越好。&lt;/li>
&lt;li>建議先實現核心功能，再逐步優化。&lt;/li>
&lt;li>中後期規格大幅調整，修改成功率偏低(尤其是&lt;strong>布局相關&lt;/strong>失敗率較高)。&lt;/li>
&lt;li>提供範例檔案有助於生成，例如：&lt;code>基於 XXX.vue 幫我寫另一個 OOO.vue 的組件&lt;/code>。&lt;/li>
&lt;/ul>
&lt;h3 id="extras">Extras&lt;/h3>
&lt;ul>
&lt;li>原型階段不建議過早拆分以免降低開發效率。&lt;/li>
&lt;li>基本成形後可封裝部分可以考慮重構為小單元，提升處理效率(減少重複生成相同部位)。&lt;/li>
&lt;li>雖然可透過 selection 的方式局部修正，但目前讓完整上下文一起參與生成較為穩健。&lt;/li>
&lt;/ul></description></item><item><title>AI - Note</title><link>https://HoshikawaRyuukou.github.io/posts/ai---note/</link><pubDate>Thu, 29 Aug 2024 13:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---note/</guid><description>&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=eraWvfD_Ihg">一小時略懂 AI｜GPT、Sora、Diffusion model、類器官智慧OI、圖靈測試、人工智慧史&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="news">News&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.aibase.com/zh/">AIbase基地 - 让更多人看到未来 通往AGI之路&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="audio">Audio&lt;/h2>
&lt;ul>
&lt;li>Suno AI&lt;/li>
&lt;/ul>
&lt;h2 id="chat-bot">Chat Bot&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://flowgpt.com/">Chat With ChatGPT bot DAN fast and free | FlowGPT&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="document">Document&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=nl-eVo1EhEQ">新世代 AI 簡報神器 Gamma&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="translation">Translation&lt;/h2>
&lt;ul>
&lt;li>GPT Translator&lt;/li>
&lt;li>&lt;a href="https://github.com/SakuraLLM/SakuraLLM">SakuraLLM/SakuraLLM&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>AI - Ollama - Note</title><link>https://HoshikawaRyuukou.github.io/posts/ai---ollama---note/</link><pubDate>Mon, 19 Aug 2024 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---ollama---note/</guid><description>&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://ollama.com/">Ollama&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="ui">UI&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo">Page Assist - A Web UI for Local AI Models&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="commands">Commands&lt;/h2>
&lt;ul>
&lt;li>ollama list : 查看以配置本地模型&lt;/li>
&lt;li>ollama run {model} : 下載/執行模型&lt;/li>
&lt;li>ollama ps : 展示目前載入的模型、它們所佔的記憶體大小以及所使用的處理器類型（GPU 或 CPU）&lt;/li>
&lt;/ul>
&lt;h2 id="use-model-from-ollama">Use model from Ollama&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://ollama.com/search">Ollama search&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="use-gguf-model-from-hugging-face-hub">Use GGUF model from Hugging Face Hub&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Run Ollama with specified model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ollama run hf.co/{username}/{repository}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Run Ollama with specified model and desired quantization&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ollama run hf.co/{username}/{repository}:{quantization}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:IQ3_M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="use-gguf-model-from-local">Use GGUF model from local&lt;/h2>
&lt;h3 id="import_gguf_to_ollamabat">import_gguf_to_ollama.bat&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bat" data-lang="bat">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">@&lt;/span>&lt;span style="color:#00a8c8">echo&lt;/span> off
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 設定本地環境，並切換到批次檔所在的目錄&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">setlocal&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">cd&lt;/span> /d &lt;span style="color:#111">%~dp0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 搜尋當前目錄中的 .gguf 檔案&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#8045ff">%%&lt;/span>f &lt;span style="color:#00a8c8">in&lt;/span> &lt;span style="color:#111">(&lt;/span>*.gguf&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#00a8c8">do&lt;/span> &lt;span style="color:#111">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 創建 Modelfile.txt 並寫入模型檔案名稱&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">echo&lt;/span> FROM &lt;span style="color:#8045ff">%%&lt;/span>~nf.gguf &lt;span style="color:#111">&amp;gt;&lt;/span> Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 打印 Modelfile.txt 的內容以供確認&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">type&lt;/span> Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 執行 ollama create 命令來包裝模型檔&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ollama create &lt;span style="color:#8045ff">%%&lt;/span>~nf -f Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 刪除 Modelfile.txt&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">del&lt;/span> Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 如果有多個 gguf 檔案，只處理第一個找到的檔案&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">goto&lt;/span> &lt;span style="color:#111">end&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">:&lt;/span>&lt;span style="color:#111">end&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 顯示完成訊息&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">echo&lt;/span> done...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 列出已經存在的模型&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ollama list 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 等待用戶確認並關閉&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">pause&lt;/span> &lt;span style="color:#111">&amp;gt;&lt;/span>nul
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>AI - Art</title><link>https://HoshikawaRyuukou.github.io/posts/ai---art/</link><pubDate>Fri, 10 Mar 2023 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---art/</guid><description>&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.chias.com.tw/category/stable-diffusion/">Stable-Diffusion 彙整 - 瑆知識&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.bilibili.com/video/BV1HDBZYBEjK">Comfyui官方客户端 desktop桌面版来了&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="社群">社群&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://civitai.com/">Civitai&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="畫廊">畫廊&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.ptsearch.info/home/">Prompt Search&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pixai.art/">PixAI.Art&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aibooru.online/">AIBooru&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://civitai.com/user/Lizardon1025/images">Lizardon1025&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="channel">Channel&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/@JackEllie/videos">杰克艾米立&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.hoshikou-ailabo.net/">星光のAIラボ&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="prompt">Prompt&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.ptsearch.info/zh-hant/articles/create_exif/">Prompt Viewer&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://danbooru.donmai.us/wiki_pages/tag_group%3Aposture">Danbooru tag group:posture&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.256pages.com/sdxl-prompts-advanced-guide-1/">SDXL Prompts 進階指南 (1) - 鏡頭視角距離&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="extra">Extra&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/leptonai/imgpilot">ImgPilot&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://huggingface.co/2vXpSwA7/iroiro-lora/tree/main">2vXpSwA7/iroiro-lora&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="stable-diffusion---lora-block-weight">Stable-Diffusion - Lora-Block-Weight&lt;/h2>
&lt;ul>
&lt;li>XERSON005:1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0&lt;/li>
&lt;li>PERSON105:1,0,0,1,1,0,0,0,1,1,1,0,0,0,0,0,0&lt;/li>
&lt;/ul></description></item></channel></rss>