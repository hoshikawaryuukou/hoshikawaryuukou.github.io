<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 識之箱庭</title><link>https://HoshikawaRyuukou.github.io/tags/ai/</link><description>Recent content in AI on 識之箱庭</description><generator>Hugo</generator><language>zh-tw</language><copyright>HoshikawaRyuukou</copyright><lastBuildDate>Fri, 29 Aug 2025 10:08:39 +0800</lastBuildDate><atom:link href="https://HoshikawaRyuukou.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>AI - Tools</title><link>https://HoshikawaRyuukou.github.io/posts/ai---tools/</link><pubDate>Fri, 29 Aug 2025 10:08:39 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---tools/</guid><description>&lt;h2 id="news"&gt;News&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.aibase.com/zh/"&gt;AIbase基地 - 让更多人看到未来 通往AGI之路&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="audio"&gt;Audio&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV17SVUznEGw"&gt;AI声音建模：MiniMax Audio 一键声音克隆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="coding"&gt;Coding&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=IqVo8V4QNm0"&gt;GitHub项目理解神器：DeepWiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/AsyncFuncAI/deepwiki-open"&gt;AsyncFuncAI/deepwiki-open&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="design"&gt;Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=c0K308mLz8U"&gt;【Lovart】設計師偷偷在用的AI 工具🫢 3分鐘搞定 Logo＋海報＋網站🔥 Laichu - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="translation"&gt;Translation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/SakuraLLM/SakuraLLM"&gt;SakuraLLM/SakuraLLM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zyddnys/manga-image-translator"&gt;zyddnys/manga-image-translator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmMaze/BallonsTranslator"&gt;dmMaze/BallonsTranslator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>AI - Art - Stable Diffusion - Inpainting</title><link>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion---inpainting/</link><pubDate>Thu, 12 Jun 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion---inpainting/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;目前並未特別使用進階修圖技巧。若圖片瑕疵可透過簡單塗色與描邊處理，即會嘗試修復。&lt;/p&gt;
&lt;p&gt;若瑕疵較嚴重，則多半直接放棄並重新生成 —— 通常下一張會更好。&lt;/p&gt;
&lt;h2 id="modification"&gt;Modification&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;圖片先送入 &lt;code&gt;img2img&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;簡單處理：使用 &lt;code&gt;Inpaint sketch&lt;/code&gt;（注意不要在 sketch 模式下直接生成）。&lt;/li&gt;
&lt;li&gt;複雜處理：使用 &lt;code&gt;photopea-embed&lt;/code&gt; 進行手動遮罩或編輯。&lt;/li&gt;
&lt;li&gt;完成後再送回 &lt;code&gt;Inpaint&lt;/code&gt; 重新生成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="generation"&gt;Generation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;採樣方法（Sampling method）：&lt;code&gt;DPM++ 2M&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;時間表類型（Schedule type）：&lt;code&gt;Karras&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;去雜強度（Denoising strength）建議範圍：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;0.2 ~ 0.3&lt;/code&gt;：保留原圖整體色彩結構，僅微調瑕疵與過渡區域。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;0.3 ~ 0.5&lt;/code&gt;：適度改變結構與細節，適合嘗試新的構圖或調整 seed 取得更好結果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>AI - Common</title><link>https://HoshikawaRyuukou.github.io/posts/ai---common/</link><pubDate>Wed, 26 Mar 2025 10:08:39 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---common/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;近幾年 AI 發展一日千里。😅&lt;/p&gt;
&lt;h2 id="guide"&gt;Guide&lt;/h2&gt;
&lt;h3 id="artificial-general-intelligenceagi"&gt;Artificial General Intelligence（AGI）&lt;/h3&gt;
&lt;p&gt;AGI 就像能像人類一樣思考和學習的 AI。與目前只能處理特定任務的 AI 不同，AGI 能適應各種情境、學習新知、獨立思考並解決問題，是人工智慧的終極目標。&lt;/p&gt;
&lt;h3 id="edge-computing"&gt;Edge Computing&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;邊緣運算&lt;/strong&gt;就是讓資料在&lt;strong&gt;靠近來源端&lt;/strong&gt;的地方處理，而非全部送往遠端伺服器（雲端）計算。例如，自駕車能即時處理感測器資料，&lt;strong&gt;更快做出決策&lt;/strong&gt;，減少延遲並提升效率。此技術廣泛應用於&lt;strong&gt;自駕車、智慧工廠、AR/VR、智慧家庭&lt;/strong&gt;等需要即時反應的領域。&lt;/p&gt;
&lt;h3 id="retrieval-augmented-generationrag"&gt;Retrieval-Augmented Generation（RAG）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;RAG 就是讓 AI「先查資料再回答」&lt;/strong&gt;，不單靠自身記憶。傳統 AI 模型可能因資訊過時而回答不準確，RAG 則會&lt;strong&gt;先查找最新資料&lt;/strong&gt;後再生成回應。這讓它特別適合&lt;strong&gt;問答系統、知識搜尋、技術支援&lt;/strong&gt;等需要&lt;strong&gt;即時可靠資訊&lt;/strong&gt;的應用。&lt;/p&gt;
&lt;h3 id="multimodal"&gt;Multimodal&lt;/h3&gt;
&lt;p&gt;多模態 AI 能同時理解和處理多種資料，如文字、圖片、聲音和影片。相較於只專注單一資料的傳統 AI，多模態 AI 能整合不同資訊，對世界的理解更全面，並執行更複雜的任務。&lt;/p&gt;
&lt;h3 id="natively-multimodal"&gt;Natively Multimodal&lt;/h3&gt;
&lt;p&gt;「原生多模態」是指模型從設計之初就支援多模態，因此能更自然、高效地跨模態理解與生成資訊。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gemini 2.0 Flash&lt;/strong&gt;   - &lt;a href="https://www.youtube.com/watch?v=w0-L2kl_3cU"&gt;用嘴 P 圖的這一天真的來了！超強多模態 Gemini AI 讓一票設計師默默把繪圖板拿起來邊啃邊思考人生 ~&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GPT 4o 原生多模態圖片生成&lt;/strong&gt;   - &lt;a href="https://www.bilibili.com/video/BV1yZZMYEEQ4"&gt;OpenAI重大更新，降维打击，自然语言绘图功能修改图片功能跨代提升，真正的多模态，从此人人都是设计师&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="model-context-protocolmcp"&gt;Model Context Protocol（MCP）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1XFhPzoEBx"&gt;Function Calling、MCP和A2A的核心原理与区别&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;模型上下文協議（MCP）由 Anthropic 提出，是一種規範 AI 模型與外部工具、資料互動的開放標準。它像 AI 的「萬用轉接頭」，透過統一規範，讓 AI 無縫存取 API、資料庫與應用程式，大幅提升開發效率與彈性。&lt;/p&gt;
&lt;h2 id="extras"&gt;Extras&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=eraWvfD_Ihg"&gt;一小時略懂 AI｜GPT、Sora、Diffusion model、類器官智慧OI、圖靈測試、人工智慧史&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=jW2cmZ-9hLk"&gt;【人工智能】模型压缩四大方法概述 | 量化、剪枝、蒸馏和二值化 | 模型瘦身 | 降低精度 | 速度提升 | 知识蒸馏 | 温度参数 | XNOR | 优缺点 | 发展方向&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1z8XpYKEnr"&gt;Gemini逆袭Controlnet？扩散模型和自回归模型的真正秘密&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>AI - Art - Stable Diffusion - Quick Start</title><link>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion---quick-start/</link><pubDate>Mon, 17 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion---quick-start/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;⚠️ 這是一篇新手導引，目的不在於精準解釋。&lt;/li&gt;
&lt;li&gt;⚠️ &lt;strong&gt;Checkpoint&lt;/strong&gt; 一般會提供推薦的參數設置，建議依據模型的特性調整，以獲得最佳效果。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="resources"&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://civitai.com/"&gt;Civitai: The Home of Open-Source Generative AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="checkpoint"&gt;Checkpoint&lt;/h2&gt;
&lt;p&gt;決定生成圖片的基礎風格。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;寫實風格 (Photorealistic)&lt;/li&gt;
&lt;li&gt;動漫風 (Anime)&lt;/li&gt;
&lt;li&gt;油畫風格 (Painting)&lt;/li&gt;
&lt;li&gt;科幻賽博龐克 (Cyberpunk)&lt;/li&gt;
&lt;li&gt;像素風格 (Pixel Art)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lora"&gt;LoRA&lt;/h2&gt;
&lt;p&gt;輕量化微調模型可額外載入來增強特定風格或角色。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;簡單的比喻來形容 LoRA 模型，那就是「濾鏡」&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="embedding"&gt;Embedding&lt;/h2&gt;
&lt;p&gt;增強對某些 Prompt 的理解。&lt;/p&gt;
&lt;h2 id="vae"&gt;VAE&lt;/h2&gt;
&lt;p&gt;提高圖片細節與顏色準確度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;📝 部分 Checkpoints 會內建（Baked）VAE，如使用外部 VAE，請確認是否需要覆蓋內建版本。&lt;/li&gt;
&lt;li&gt;⚠️ 如果發現圖片的型都對，但只有顏色壞掉，通常都是 VAE 的問題。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="resolutions"&gt;Resolutions&lt;/h2&gt;
&lt;p&gt;不同種類的 Checkpoints 建議的解析度會有所不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SD 1.5
&lt;ul&gt;
&lt;li&gt;512 x 512 : 1:1&lt;/li&gt;
&lt;li&gt;512 X 768 : 2:3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SDXL
&lt;ul&gt;
&lt;li&gt;640 x 1536 = 5:12&lt;/li&gt;
&lt;li&gt;768 x 1344 = 4:7&lt;/li&gt;
&lt;li&gt;832 x 1216 = 13:19&lt;/li&gt;
&lt;li&gt;896 x 1152 = 7:9&lt;/li&gt;
&lt;li&gt;1024 x 1024 = 1:1&lt;/li&gt;
&lt;li&gt;1024 x 1536 = 2:3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sampler--schedule"&gt;Sampler + Schedule&lt;/h2&gt;
&lt;p&gt;Sampler 是從雜訊圖到成品的&lt;strong&gt;去噪算法&lt;/strong&gt;。&lt;/p&gt;</description></item><item><title>AI - Art - Stable Diffusion - Env</title><link>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion---env/</link><pubDate>Mon, 10 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion---env/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;⚠️ 以下皆須安裝指定版本不可貿然升級。&lt;/p&gt;
&lt;p&gt;目前產圖只使用到 Forge + SDXL 。&lt;/p&gt;
&lt;h2 id="local-deployment"&gt;Local deployment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nvidia 驅動更新至最新&lt;/li&gt;
&lt;li&gt;檢查顯卡支援的最高 cuda 支援: &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安裝 &lt;a href="https://developer.nvidia.com/cuda-12-1-0-download-archive"&gt;CUDA 12.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;顯示 cuda 編譯工具的版本信息: &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安裝 [&lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;stable-diffusion-webui-forge&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="google-colab"&gt;Google Colab&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/gutris1/segsmaker"&gt;gutris1/segsmaker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;配置 Civitai API Key
&lt;ul&gt;
&lt;li&gt;Civitai 網站 Menu &amp;gt; Account Settings(齒輪 icon) &amp;gt; API Keys&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="extensions"&gt;Extensions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DominikDoom/a1111-sd-webui-tagcomplete"&gt;DominikDoom/a1111-sd-webui-tagcomplete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Physton/sd-webui-prompt-all-in-one"&gt;Physton/sd-webui-prompt-all-in-one&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Bing-su/adetailer"&gt;Bing-su/adetailer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yankooliveira/sd-webui-photopea-embed"&gt;yankooliveira/sd-webui-photopea-embed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>AI - Art - Stable Diffusion</title><link>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion/</link><pubDate>Thu, 06 Feb 2025 10:00:00 +0800</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---art---stable-diffusion/</guid><description>&lt;h2 id="quick-chat"&gt;Quick Chat&lt;/h2&gt;
&lt;p&gt;目前 Stable Diffusion 只拿來自娛自樂 😃。&lt;/p&gt;
&lt;h2 id="core-working-principles"&gt;Core Working Principles&lt;/h2&gt;
&lt;p&gt;Stable Diffusion 主要包含三個核心技術：&lt;/p&gt;
&lt;h3 id="前向擴散forward-diffusion"&gt;前向擴散（Forward Diffusion）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;先從大量圖片資料集中學習圖片特徵。&lt;/li&gt;
&lt;li&gt;然後，系統會逐步加入高斯雜訊（Gaussian Noise），使圖片變得模糊、無法辨識。&lt;/li&gt;
&lt;li&gt;最後，這個過程會讓圖片變成完全的純雜訊（random noise）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="反向去噪reverse-denoising--u-net"&gt;反向去噪（Reverse Denoising / U-Net）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion 學習如何逆向去噪，一步步從雜訊還原出清晰的圖片。&lt;/li&gt;
&lt;li&gt;這部分的關鍵是 U-Net 神經網路架構，它可以在多層次的細節中，捕捉圖片的各種特徵。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="文本引導text-conditioning--clip"&gt;文本引導（Text Conditioning / CLIP）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion 之所以能生成符合指令的圖片，是因為它使用了CLIP（Contrastive Language-Image Pretraining）。&lt;/li&gt;
&lt;li&gt;CLIP 會將文字轉換成向量表示（latent embeddings），這些向量再指導模型生成符合描述的圖像。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="diagram"&gt;Diagram&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/"&gt;Improving Diffusion Models as an Alternative To GANs, Part 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
 &lt;img src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/Generation-with-Diffusion-Models.png" alt="123"&gt;

&lt;/p&gt;
&lt;h2 id="extras"&gt;Extras&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pttweb.cc/bbs/C_Chat/M.1730732828.A.70C"&gt;Re: [問題] AI 風格怎麼了嗎？為什麼容易膩？ - 看板C_Chat - PTT網頁版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>