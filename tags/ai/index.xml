<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 識之箱庭</title><link>https://HoshikawaRyuukou.github.io/tags/ai/</link><description>Recent content in AI on 識之箱庭</description><generator>Hugo</generator><language>zh-tw</language><copyright>HoshikawaRyuukou</copyright><lastBuildDate>Mon, 17 Feb 2025 20:40:00 +0000</lastBuildDate><atom:link href="https://HoshikawaRyuukou.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>AI - Stable diffusion - Quick Start</title><link>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---quick-start/</link><pubDate>Mon, 17 Feb 2025 20:40:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---quick-start/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;ul>
&lt;li>⚠️ 這是一篇新手導向的筆記，目的不在於精準解釋。&lt;/li>
&lt;li>⚠️ 環境配置請參考 &lt;code>AI - Stable diffusion - Environment&lt;/code>&lt;/li>
&lt;li>⚠️ &lt;strong>Checkpoint&lt;/strong> 一般會提供推薦的參數設置，建議依據模型的特性調整，以獲得最佳效果。&lt;/li>
&lt;/ul>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://civitai.com/">Civitai: The Home of Open-Source Generative AI&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="checkpoint">Checkpoint&lt;/h2>
&lt;p>決定生成圖片的基礎風格。&lt;/p>
&lt;ul>
&lt;li>寫實風格 (Photorealistic)&lt;/li>
&lt;li>動漫風 (Anime)&lt;/li>
&lt;li>油畫風格 (Painting)&lt;/li>
&lt;li>科幻賽博龐克 (Cyberpunk)&lt;/li>
&lt;li>像素風格 (Pixel Art)&lt;/li>
&lt;/ul>
&lt;h2 id="lora">LoRA&lt;/h2>
&lt;p>輕量化微調模型可額外載入來增強特定風格或角色。&lt;/p>
&lt;ul>
&lt;li>簡單的比喻來形容 LoRA 模型，那就是「濾鏡」&lt;/li>
&lt;/ul>
&lt;h2 id="embedding">Embedding&lt;/h2>
&lt;p>增強對某些 Prompt 的理解。&lt;/p>
&lt;h2 id="vae">VAE&lt;/h2>
&lt;p>提高圖片細節與顏色準確度。&lt;/p>
&lt;ul>
&lt;li>📝 部分 Checkpoints 會內建（Baked）VAE，如使用外部 VAE，請確認是否需要覆蓋內建版本。&lt;/li>
&lt;li>⚠️ 如果發現圖片的型都對，但只有顏色壞掉，通常都是 VAE 的問題。&lt;/li>
&lt;/ul>
&lt;h2 id="sampler--schedule">Sampler + Schedule&lt;/h2>
&lt;p>Sampler 是從雜訊圖到成品的&lt;strong>去噪算法&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>快速收斂&lt;/strong> – 能迅速找到解答，適合驗證創意和想法。&lt;/li>
&lt;li>&lt;strong>高品質收斂&lt;/strong> – 需較長時間，但能提供更精確結果。&lt;/li>
&lt;li>&lt;strong>無固定收斂&lt;/strong> – 無明確收斂條件，為創新提供更大空間。&lt;/li>
&lt;/ul>
&lt;p>Schedule 是從雜訊圖到成品的&lt;strong>去噪程度&lt;/strong>。&lt;/p></description></item><item><title>AI - Stable diffusion - Extensions</title><link>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---extensions/</link><pubDate>Mon, 10 Feb 2025 22:16:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---extensions/</guid><description>&lt;h2 id="sd-webui-photopea-embed">sd-webui-photopea-embed&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Physton/sd-webui-prompt-all-in-one/tree/main">yankooliveira/sd-webui-photopea-embed&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="lora-block-weight">Lora-Block-Weight&lt;/h2>
&lt;ul>
&lt;li>XERSON005:1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0&lt;/li>
&lt;li>PERSON105:1,0,0,1,1,0,0,0,1,1,1,0,0,0,0,0,0&lt;/li>
&lt;/ul></description></item><item><title>AI - Stable diffusion - GUI</title><link>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---gui/</link><pubDate>Mon, 10 Feb 2025 21:16:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---gui/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;p>建議新手直接從 Forge 入門即可&lt;/p>
&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.bilibili.com/video/BV1HDBZYBEjK">Comfyui官方客户端 desktop桌面版来了&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="automatic1111">&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Automatic1111&lt;/a>&lt;/h2>
&lt;p>是最早推出的圖形使用者介面之一，為使用者提供了直觀且功能豐富的操作平台。由於其開源性質和強大的社群支持，許多初學者和開發者選擇從 Automatic1111 入手，逐步熟悉 Stable Diffusion 的各項功能和應用。&lt;/p>
&lt;p>
 &lt;img src="https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/refs/heads/master/screenshot.png" alt="Automatic1111 Screenshot">

&lt;/p>
&lt;h2 id="forge">&lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge">Forge&lt;/a>&lt;/h2>
&lt;p>基於 Automatic1111 進行了多項優化&lt;/p>
&lt;ul>
&lt;li>記憶體控制優化且推理速度提升&lt;/li>
&lt;li>算法優化&lt;/li>
&lt;li>新增取樣器&lt;/li>
&lt;li>簡化的命令標誌&lt;/li>
&lt;/ul>
&lt;p>介面與 Automatic1111 高度相似，基本能無痛從 Automatic1111 轉移。&lt;/p>
&lt;h2 id="comfyui">&lt;a href="https://github.com/comfyanonymous/ComfyUI">ComfyUI&lt;/a>&lt;/h2>
&lt;p>是一個開源的節點式圖形介面，允許使用者通過直觀的節點系統設計和執行複雜的工作流程。&lt;/p>
&lt;p>
 &lt;img src="https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe" alt="ComfyUI Screenshot">

&lt;/p></description></item><item><title>AI - Stable diffusion - Environment</title><link>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---environment/</link><pubDate>Mon, 10 Feb 2025 20:13:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---environment/</guid><description>&lt;h2 id="local-deployment">Local deployment&lt;/h2>
&lt;p>⚠️ 以下皆須安裝指定版本不可貿然升級&lt;/p>
&lt;ul>
&lt;li>nvidia 驅動更新至最新&lt;/li>
&lt;li>cuda: &lt;a href="https://developer.nvidia.com/cuda-12-1-0-download-archive">CUDA 12.1&lt;/a>
&lt;ul>
&lt;li>檢查顯卡支援的最高 cuda 支援: &lt;code>nvidia-smi&lt;/code>&lt;/li>
&lt;li>顯示CUDA編譯工具的版本信息: &lt;code>nvcc --version&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Python: &lt;a href="https://www.python.org/ftp/python/3.10.11/python-3.10.11-amd64.exe">Python 3.10.11&lt;/a>
&lt;ul>
&lt;li>&lt;code>python --version&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Git: &lt;a href="https://git-fork.com/">Fork&lt;/a>&lt;/li>
&lt;li>GUI: &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge">lllyasviel/stable-diffusion-webui-forge&lt;/a>&lt;/li>
&lt;li>clone 上述專案，執行 &lt;code>webui.bat&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="google-colab">Google Colab&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gutris1/segsmaker">gutris1/segsmaker&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cagliostrolab/forge-colab">cagliostrolab/forge-colab&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>AI - Stable diffusion - Note</title><link>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---note/</link><pubDate>Thu, 06 Feb 2025 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---note/</guid><description>&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.chias.com.tw/category/stable-diffusion/">Stable-Diffusion 彙整 - 瑆知識&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="community">Community&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://civitai.com/">Civitai&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.qpipi.com/">Qpipi_AI绘画社区和SD模型&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="channel">Channel&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/@JackEllie/videos">杰克艾米立&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.hoshikou-ailabo.net/">星光のAIラボ&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="gallery">Gallery&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.ptsearch.info/home/">Prompt Search&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pixai.art/">PixAI.Art&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aibooru.online/">AIBooru&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://civitai.com/user/Lizardon1025/images">Lizardon1025&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>AI - Stable diffusion - Prompts</title><link>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---prompts/</link><pubDate>Wed, 05 Feb 2025 20:13:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---prompts/</guid><description>&lt;h2 id="prompt">Prompt&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.ptsearch.info/zh-hant/articles/create_exif/">Prompt Viewer&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://danbooru.donmai.us/wiki_pages/tag_group%3Aposture">Danbooru tag group:posture&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.256pages.com/sdxl-prompts-advanced-guide-1/">SDXL Prompts 進階指南 (1) - 鏡頭視角距離&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://danbooru.donmai.us/artists">Artists | Danbooru&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="artist">Artist&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://danbooru.donmai.us/posts?tags=akitetsu">Akitetsu | Danbooru&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://danbooru.donmai.us/posts?tags=sakamoto_masaru">Sakamoto Masaru | Danbooru&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://danbooru.donmai.us/posts?tags=fight_yoghurt">Fight Yoghurt | Danbooru&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://danbooru.donmai.us/posts?tags=reia_76">Reia 76 | Danbooru&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>AI - Stable diffusion - CheckPoints</title><link>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---checkpoints/</link><pubDate>Tue, 04 Feb 2025 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---stable-diffusion---checkpoints/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;ul>
&lt;li>⚠️ 以下主題專注於二次元/動漫風格圖像生成&lt;/li>
&lt;/ul>
&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.pttweb.cc/bbs/C_Chat/M.1730732828.A.70C">Re: [問題] AI 風格怎麼了嗎？為什麼容易膩？ - 看板C_Chat - PTT網頁版&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="sd15">SD1.5&lt;/h2>
&lt;p>在性能和穩定性上提升很多，社群迎來爆發式成長。&lt;/p>
&lt;h3 id="milestoneevents">Milestone/Events&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Waifu Diffusion&lt;/strong>：這是一個基於 Stable Diffusion 的模型，專注於生成二次元風格的圖像。該模型使用 Danbooru 資料集進行訓練，適合生成各類動漫風格的圖像。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>NovelAI 模型外洩（NAI）&lt;/strong>：NovelAI 是一個提供 AI 輔助創作的服務平台，其專注於二次元圖像生成的模型曾發生外洩事件。該模型同樣使用 Danbooru 資料集進行訓練。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Anything 系列模型&lt;/strong>：Anything V3 和 V4 是專注於二次元圖像生成的模型，具有較高的生成質量和風格多樣性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ChilloutMix&lt;/strong>：這是一個專注於生成寫實風格圖像的模型，能夠生成高品質的寫實人物圖像。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="resolutions">Resolutions&lt;/h3>
&lt;ul>
&lt;li>512 x 512 : 1:1&lt;/li>
&lt;li>512 X 768 : 2:3&lt;/li>
&lt;/ul>
&lt;h2 id="sdxl">SDXL&lt;/h2>
&lt;p>相比於 SD1.5 在多方面有顯著的提升&lt;/p>
&lt;ul>
&lt;li>&lt;strong>更大的模型規模&lt;/strong>：SDXL 的參數量遠超 SD1.5，這使其能夠捕捉更複雜的圖像特徵。&lt;/li>
&lt;li>&lt;strong>更高分辨率&lt;/strong>：SDXL 支持更高分辨率的圖像生成。&lt;/li>
&lt;li>&lt;strong>雙模型架構&lt;/strong>：SDXL 採用雙模型架構，包含一個基礎模型和一個精煉模型。基礎模型生成初步圖像，精煉模型進一步提升細節和質量，這種分工協作顯著提升了生成效果。&lt;/li>
&lt;li>&lt;strong>更強的文本理解能力&lt;/strong>：SDXL 在理解複雜提示詞方面表現更好，能更準確地將文本描述轉化為圖像內容，減少誤解和偏差。&lt;/li>
&lt;/ul>
&lt;h3 id="milestoneevents-1">Milestone/Events&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Pony Diffusion&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>訓練流程有使用審美分級標籤。&lt;/li>
&lt;li>原版 Pony 生成的結果接近歐美審美。&lt;/li>
&lt;li>Pony 系明顯缺點用色偏暗偏髒。&lt;/li>
&lt;li>Pony 在「多人互動」方面與 nsfw 表現很好。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>NovelAI 3 (NAI3)&lt;/strong>：&lt;/p></description></item><item><title>AI - Ollama - Google Colab + ngrok</title><link>https://HoshikawaRyuukou.github.io/posts/ai---ollama---google-colab-+-ngrok/</link><pubDate>Thu, 16 Jan 2025 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---ollama---google-colab-+-ngrok/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;p>參考以下教學&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=ZOCY61424JI">十分钟部署本地离线免费大模型！&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=JfI3K3HwQuI">Ngrok + Ollama | 在世界任何地方与localhost开源大模型聊天&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.gopenai.com/free-inference-is-all-i-need-how-to-run-large-language-models-for-free-using-google-colab-fe961e86503b">Free Inference Is All I Need: How to Run Large Language Models for Free Using Google Colab&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;ul>
&lt;li>註冊 &lt;a href="https://ngrok.com/">ngrok&lt;/a> 帳號，取得 token ( ngrok &amp;gt; Your Authtoken )&lt;/li>
&lt;li>將 token 填至 colab &amp;gt; Secret
&lt;ul>
&lt;li>name : NGROK_AUTH&lt;/li>
&lt;li>value : token&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>本機端使用 &lt;a href="https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo">Page Assist - A Web UI for Local AI Models&lt;/a> 與 Ollama 互動&lt;/li>
&lt;/ul>
&lt;h2 id="steps">Steps&lt;/h2>
&lt;h3 id="安裝必要工具">安裝必要工具&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!sudo apt-get install -y pciutils
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>!curl https://ollama.ai/install.sh &lt;span style="color:#111">|&lt;/span> sh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>!pip install pyngrok
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>安裝 pciutils&lt;/strong>: 提供硬件檢測和配置工具，用於檢查和診斷 GPU 設置。&lt;/li>
&lt;li>&lt;strong>安裝 Ollama&lt;/strong>: 下載並執行 Ollama 的安裝腳本。&lt;/li>
&lt;li>&lt;strong>安裝 pyngrok&lt;/strong>: 用於創建到本地服務的反向代理，從而將本地服務器公開到互聯網。&lt;/li>
&lt;/ul>
&lt;h3 id="啟動-ollama-服務器">啟動 Ollama 服務器&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">os&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">threading&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">subprocess&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">requests&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">pyngrok&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">ngrok&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#111">conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> &lt;span style="color:#111">google.colab&lt;/span> &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#111">userdata&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">def&lt;/span> &lt;span style="color:#75af00">ollama&lt;/span>&lt;span style="color:#111">():&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_HOST&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;0.0.0.0:11434&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_ORIGINS&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;*&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">os&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">environ&lt;/span>&lt;span style="color:#111">[&lt;/span>&lt;span style="color:#d88200">&amp;#39;OLLAMA_KEEP_ALIVE&amp;#39;&lt;/span>&lt;span style="color:#111">]&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#d88200">&amp;#39;-1&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#111">subprocess&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Popen&lt;/span>&lt;span style="color:#111">([&lt;/span>&lt;span style="color:#d88200">&amp;#34;ollama&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;serve&amp;#34;&lt;/span>&lt;span style="color:#111">])&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_thread&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">threading&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">Thread&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#111">target&lt;/span>&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#111">ollama&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_thread&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">start&lt;/span>&lt;span style="color:#111">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>配置環境變量&lt;/strong>：
&lt;ul>
&lt;li>OLLAMA_HOST: 指定服務器的主機和端口，這裡為 0.0.0.0:11434，表示本地所有網絡接口。&lt;/li>
&lt;li>OLLAMA_ORIGINS: 設置跨域資源共享 (CORS) 的允許範圍。&lt;/li>
&lt;li>OLLAMA_KEEP_ALIVE: 保持服務器活躍的時長（-1 表示無限）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>啟動 Ollama 服務器&lt;/strong>：使用 subprocess 啟動 Ollama 的服務模式。&lt;/li>
&lt;li>&lt;strong>使用執行緒運行服務器&lt;/strong>：確保主程序不被阻塞，允許服務器在後台運行。&lt;/li>
&lt;/ul>
&lt;h3 id="下載模型">下載模型&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://ollama.com/search">Ollama search&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama pull &lt;span style="color:#f92672">{&lt;/span>model_name&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="公開服務">公開服務&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">conf&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">get_default&lt;/span>&lt;span style="color:#111">()&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">auth_token&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">userdata&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">get&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#39;NGROK_AUTH&amp;#39;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">ollama_tunnel&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ngrok&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">connect&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">&amp;#34;11434&amp;#34;&lt;/span>&lt;span style="color:#111">,&lt;/span> &lt;span style="color:#d88200">&amp;#34;http&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">public_url&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#111">ollama_tunnel&lt;/span>&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#111">public_url&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">print&lt;/span>&lt;span style="color:#111">(&lt;/span>&lt;span style="color:#d88200">f&lt;/span>&lt;span style="color:#d88200">&amp;#34;Public URL: &lt;/span>&lt;span style="color:#d88200">{&lt;/span>&lt;span style="color:#111">public_url&lt;/span>&lt;span style="color:#d88200">}&lt;/span>&lt;span style="color:#d88200">&amp;#34;&lt;/span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>配置 ngrok 驗證令牌&lt;/strong>：使用用戶提供的 NGROK_AUTH 確保 Tunnel 服務的授權。&lt;/li>
&lt;li>&lt;strong>創建 ngrok Tunnel&lt;/strong>： 將本地服務器（11434 端口）通過 HTTP 隧道公開到互聯網。&lt;/li>
&lt;li>&lt;strong>獲取公開 URL&lt;/strong>： 輸出 Tunnel 的公開 URL，便於遠程訪問 Ollama 服務。&lt;/li>
&lt;/ul>
&lt;h3 id="列出可用模型">列出可用模型&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama list
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="執行模型">執行模型&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>!ollama run &lt;span style="color:#f92672">{&lt;/span>model_name&lt;span style="color:#f92672">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="透過-page-assist-訪問">透過 Page Assist 訪問&lt;/h3>
&lt;ul>
&lt;li>於 Page Assist 設置 public_url&lt;/li>
&lt;li>訪問 public_url 並點擊 visit site，否則 Page Assist 偵測不到遠端 ollama&lt;/li>
&lt;/ul></description></item><item><title>AI - Prompt Engineering</title><link>https://HoshikawaRyuukou.github.io/posts/ai---prompt-engineering/</link><pubDate>Mon, 18 Nov 2024 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---prompt-engineering/</guid><description>&lt;h2 id="quick-chat">Quick Chat&lt;/h2>
&lt;p>提示工程是一種專注於設計和優化輸入提示的&lt;strong>操作者技術&lt;/strong>，旨在不改變模型的前提下，通過精心設計提示來提升生成式人工智慧（如大型語言模型，LLMs）的輸出品質。這種技術能幫助模型更準確地理解用戶意圖並生成符合需求的回應。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>提示（Prompt）&lt;/strong>：提示是提供給 AI 模型的輸入內容，如問題、命令或指示。高品質提示是生成高品質輸出的關鍵。&lt;/li>
&lt;/ul>
&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://medium.com/@micky2428/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B-prompt-engineering-%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E8%88%87ai%E5%B0%8D%E8%A9%B1-c4e6501c9bfd">提示工程(Prompt Engineering)：如何有效與AI對話&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="practice---github-copilot">Practice - Github Copilot&lt;/h2>
&lt;h3 id="domain-model">Domain Model&lt;/h3>
&lt;ul>
&lt;li>偏重邏輯，提示詞規格越詳細越好。&lt;/li>
&lt;/ul>
&lt;h3 id="vue-component">Vue Component&lt;/h3>
&lt;ul>
&lt;li>前端組件，初期提示詞規格未必越詳細越好。&lt;/li>
&lt;li>建議先實現核心功能，再逐步優化。&lt;/li>
&lt;li>中後期規格大幅調整，修改成功率偏低(尤其是&lt;strong>布局相關&lt;/strong>失敗率較高)。&lt;/li>
&lt;li>提供範例檔案有助於生成，例如：&lt;code>基於 XXX.vue 幫我寫另一個 OOO.vue 的組件&lt;/code>。&lt;/li>
&lt;/ul>
&lt;h3 id="extras">Extras&lt;/h3>
&lt;ul>
&lt;li>原型階段不建議過早拆分以免降低開發效率。&lt;/li>
&lt;li>基本成形後可封裝部分可以考慮重構為小單元，提升處理效率(減少重複生成相同部位)。&lt;/li>
&lt;li>雖然可透過 selection 的方式局部修正，但目前讓完整上下文一起參與生成較為穩健。&lt;/li>
&lt;/ul></description></item><item><title>AI - Note</title><link>https://HoshikawaRyuukou.github.io/posts/ai---note/</link><pubDate>Thu, 29 Aug 2024 13:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---note/</guid><description>&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=eraWvfD_Ihg">一小時略懂 AI｜GPT、Sora、Diffusion model、類器官智慧OI、圖靈測試、人工智慧史&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=jW2cmZ-9hLk">【人工智能】模型压缩四大方法概述 | 量化、剪枝、蒸馏和二值化 | 模型瘦身 | 降低精度 | 速度提升 | 知识蒸馏 | 温度参数 | XNOR | 优缺点 | 发展方向&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="news">News&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.aibase.com/zh/">AIbase基地 - 让更多人看到未来 通往AGI之路&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="audio">Audio&lt;/h2>
&lt;ul>
&lt;li>Suno AI&lt;/li>
&lt;/ul>
&lt;h2 id="chat-bot">Chat Bot&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://flowgpt.com/">Chat With ChatGPT bot DAN fast and free | FlowGPT&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="document">Document&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=nl-eVo1EhEQ">新世代 AI 簡報神器 Gamma&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="translation">Translation&lt;/h2>
&lt;ul>
&lt;li>GPT Translator&lt;/li>
&lt;li>&lt;a href="https://github.com/SakuraLLM/SakuraLLM">SakuraLLM/SakuraLLM&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>AI - Ollama - Note</title><link>https://HoshikawaRyuukou.github.io/posts/ai---ollama---note/</link><pubDate>Mon, 19 Aug 2024 21:11:00 +0000</pubDate><guid>https://HoshikawaRyuukou.github.io/posts/ai---ollama---note/</guid><description>&lt;h2 id="guide">Guide&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://ollama.com/">Ollama&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="ui">UI&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo">Page Assist - A Web UI for Local AI Models&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="commands">Commands&lt;/h2>
&lt;ul>
&lt;li>ollama list : 查看以配置本地模型&lt;/li>
&lt;li>ollama run {model} : 下載/執行模型&lt;/li>
&lt;li>ollama ps : 展示目前載入的模型、它們所佔的記憶體大小以及所使用的處理器類型（GPU 或 CPU）&lt;/li>
&lt;/ul>
&lt;h2 id="use-model-from-ollama">Use model from Ollama&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://ollama.com/search">Ollama search&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="use-gguf-model-from-hugging-face-hub">Use GGUF model from Hugging Face Hub&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Run Ollama with specified model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ollama run hf.co/{username}/{repository}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Run Ollama with specified model and desired quantization&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># ollama run hf.co/{username}/{repository}:{quantization}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:IQ3_M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="use-gguf-model-from-local">Use GGUF model from local&lt;/h2>
&lt;h3 id="import_gguf_to_ollamabat">import_gguf_to_ollama.bat&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bat" data-lang="bat">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">@&lt;/span>&lt;span style="color:#00a8c8">echo&lt;/span> off
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 設定本地環境，並切換到批次檔所在的目錄&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">setlocal&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">cd&lt;/span> /d &lt;span style="color:#111">%~dp0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 搜尋當前目錄中的 .gguf 檔案&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">for&lt;/span> &lt;span style="color:#8045ff">%%&lt;/span>f &lt;span style="color:#00a8c8">in&lt;/span> &lt;span style="color:#111">(&lt;/span>*.gguf&lt;span style="color:#111">)&lt;/span> &lt;span style="color:#00a8c8">do&lt;/span> &lt;span style="color:#111">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 創建 Modelfile.txt 並寫入模型檔案名稱&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">echo&lt;/span> FROM &lt;span style="color:#8045ff">%%&lt;/span>~nf.gguf &lt;span style="color:#111">&amp;gt;&lt;/span> Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 打印 Modelfile.txt 的內容以供確認&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">type&lt;/span> Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 執行 ollama create 命令來包裝模型檔&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ollama create &lt;span style="color:#8045ff">%%&lt;/span>~nf -f Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 刪除 Modelfile.txt&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">del&lt;/span> Modelfile.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">REM 如果有多個 gguf 檔案，只處理第一個找到的檔案&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00a8c8">goto&lt;/span> &lt;span style="color:#111">end&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#111">:&lt;/span>&lt;span style="color:#111">end&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 顯示完成訊息&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">echo&lt;/span> done...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 列出已經存在的模型&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ollama list 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">REM 等待用戶確認並關閉&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00a8c8">pause&lt;/span> &lt;span style="color:#111">&amp;gt;&lt;/span>nul
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item></channel></rss>